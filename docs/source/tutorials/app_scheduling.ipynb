{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "Application Level Scheduling\n",
    "\n",
    "RADICAL-Pilot (RP) by default uses its internal scheduler to efficiently place tasks on the available cluster resources.  Some use cases however require more fine grained and/or explicit control over task placement.  RP supports that functionality with *__application level scheduling__*.  In that case the pilot will report about the available nodes and resources, but will leave it to the application to assign resources to the tasks.  A number of API functions are provided to simplify the development of such application level schedulers, and this tutorial will demonstrate their use.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__Note:__ At the moment it is not possible to mix application level scheduling and RP's internal scheduling in the same session.\n",
    "__Note:__ Application level scheduling is only supported for executable tasks, RAPTOR tasks will ignore any related settings.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "## Notation\n",
    "\n",
    "The following terms will be used throughout this tutorial:\n",
    "\n",
    "  - *__task:__* an exectable piece of work comprised of one or more processes, all running the same executable on a dedicated set of resources.\n",
    "  - *__rank:__* one of theprocesses which comprise a running task.  The term _rank_ is frequently used for MPI applications, but we use it generically for any task which uses multiprocessing.\n",
    "  - *__slot:__* the set of resources which are assigned to a single _rank_, i.e., to a single task process. Note that each _rank_ can utilize multiple cores and/or GPUs, usually by support of libraries and frameworks such as OpenMP, CUDA, OpenCL etc.\n",
    "  - *__occupation:__* the portion of a resource assigned to a _rank_.  For example, two ranks could share a GPU, and then each of the ranks would get `occupation=0.5` assigned for that GPU.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Pilot Resources\n",
    "\n",
    "We will first start a normal RP session and submit a pilot.  We then wait for the pilot to become active and inspect its resources by retrieving its nodelist which at that point will be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import radical.pilot as rp\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[94mnew session: \u001b[39m\u001b[0m[rp.session.thinkie.merzky.020127.0000]\u001b[39m\u001b[0m\u001b[94m                           \\\n",
      "[tcp://10.0.0.39:10001]\u001b[39m\u001b[0m\u001b[92m                                          ok\n",
      "\u001b[94mcreate pilot manager\u001b[39m\u001b[0m\u001b[92m                                                          ok\n",
      "\u001b[94mcreate task manager\u001b[39m\u001b[0m\u001b[92m                                                           ok\n",
      "\u001b[94msubmit 1 pilot(s)\u001b[39m\u001b[0m\n",
      "\u001b[92m                        oklhost           4 nodes\u001b[39m\u001b[0m\n",
      "\u001b[39m\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 4 nodes\n"
     ]
    }
   ],
   "source": [
    "session = rp.Session()\n",
    "\n",
    "pmgr = rp.PilotManager(session)\n",
    "tmgr = rp.TaskManager(session)\n",
    "\n",
    "pilot = pmgr.submit_pilots(rp.PilotDescription(\n",
    "    {'resource': 'local.localhost',\n",
    "     'runtime' : 60,\n",
    "     'nodes'   : 4}))\n",
    "\n",
    "tmgr.add_pilots(pilot)\n",
    "pilot.wait([rp.PMGR_ACTIVE, rp.FAILED])\n",
    "\n",
    "assert pilot.state == rp.PMGR_ACTIVE\n",
    "\n",
    "nodelist = pilot.nodelist\n",
    "print('got %d nodes' % len(nodelist.nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nodelist (type: `rp.NodeList`) has the following attributes:\n",
    "\n",
    "  - `uniform`: boolean, indicates if the nodes have identical resources\n",
    "  - `cores_per_node`, `gpus_per_node`, `mem_per_node`, `lfs_per_node`: amount of resources per node.  Those attributes will be `None` for non-uniform nodelists.\n",
    "  - `nodes`: the actual list of nodes.\n",
    "\n",
    "Let's inspect one of the nodes (`nodeslist.nodes[0]`).  A node in the nodelist has the type `rp.NodeResource` with the following attributes:\n",
    "\n",
    "  - `index`: unique node identifier used within RP\n",
    "  - `name`: node name (does not need to be unique!)\n",
    "  - `mem`: available memory (in MB)\n",
    "  - `lfs`: available disk storage (in MB)\n",
    "  - `cores`: available CPU cores and their occupation\n",
    "  - `gpus`: available GPUs and their occupation.\n",
    "\n",
    "The core and gpu information are constructed of an integer (the resource index) and a float (the resource occupation where `0.0` is *not used* and `1.0` is *fully used*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#nodes:  4\n",
      "{'cores': [{'index': 0, 'occupation': 0.0},\n",
      "           {'index': 1, 'occupation': 0.0},\n",
      "           {'index': 2, 'occupation': 0.0},\n",
      "           {'index': 3, 'occupation': 0.0},\n",
      "           {'index': 4, 'occupation': 0.0},\n",
      "           {'index': 5, 'occupation': 0.0},\n",
      "           {'index': 6, 'occupation': 0.0},\n",
      "           {'index': 7, 'occupation': 0.0},\n",
      "           {'index': 8, 'occupation': 0.0},\n",
      "           {'index': 9, 'occupation': 0.0},\n",
      "           {'index': 10, 'occupation': 0.0},\n",
      "           {'index': 11, 'occupation': 0.0}],\n",
      " 'gpus': [{'index': 0, 'occupation': 0.0}],\n",
      " 'index': 0,\n",
      " 'lfs': 1000000,\n",
      " 'mem': 65536,\n",
      " 'name': 'localhost'}\n"
     ]
    }
   ],
   "source": [
    "# inspect one node\n",
    "print('#nodes: ', len(nodelist.nodes))\n",
    "\n",
    "node = nodelist.nodes[0]\n",
    "pprint.pprint(node.as_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `nodelist` class exposed the method `find_slots(rr: RankRequirements, n_slots: int=1)` which will return a set of resources, one for each task rank. Let us disect what this means: \n",
    "\n",
    "  - a _rank_ is a process which is part of the set of processes which comprise a task.  The term _rank_ is frequently used for MPI applications, but we use it generically for any task which uses multiprocessing.   \n",
    "  - a `slot` is defined as a set of resources which are assigned to a single _rank_, i.e., to a single task process. Note that each _rank_ can utilize multiple cores and/or GPUs, usually by support of libraries and frameworks such as OpenMP, CUDA, OpenCL etc.\n",
    "  - `occupation` is defined as the portion of a resource assigned to a _rank_.  For example, two ranks could share a GPU, and then each of the ranks would get `occupation=0.5` assigned for that GPU.\n",
    "\n",
    "The slot for the simplest possible task (in RP) would just allocate one core - say core with index `3` on the first node:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cores': [{'index': 3, 'occupation': 1.0}],\n",
      " 'gpus': [],\n",
      " 'lfs': 0,\n",
      " 'mem': 0,\n",
      " 'node_index': 0,\n",
      " 'node_name': 'localhost',\n",
      " 'version': 1}\n"
     ]
    }
   ],
   "source": [
    "slot_1 = rp.Slot(node_index=0, node_name='localhost', cores=[3])\n",
    "pprint.pprint(slot_1.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simpler way to obtain that slot is to let the node's `NodeResource` find it for you - that will though return the first available core, not number 3 as before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cores': [{'index': 0, 'occupation': 1.0}],\n",
      " 'gpus': [],\n",
      " 'lfs': 0,\n",
      " 'mem': 0,\n",
      " 'node_index': 0,\n",
      " 'node_name': 'localhost',\n",
      " 'version': 1}\n"
     ]
    }
   ],
   "source": [
    "rr = rp.RankRequirements(n_cores=1)\n",
    "slot_2 = node.find_slot(rr)\n",
    "pprint.pprint(slot_2.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now check the node, we will see that the resource occupation of the first core changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cores': [{'index': 0, 'occupation': 1.0},\n",
      "           {'index': 1, 'occupation': 0.0},\n",
      "           {'index': 2, 'occupation': 0.0},\n",
      "           {'index': 3, 'occupation': 0.0},\n",
      "           {'index': 4, 'occupation': 0.0},\n",
      "           {'index': 5, 'occupation': 0.0},\n",
      "           {'index': 6, 'occupation': 0.0},\n",
      "           {'index': 7, 'occupation': 0.0},\n",
      "           {'index': 8, 'occupation': 0.0},\n",
      "           {'index': 9, 'occupation': 0.0},\n",
      "           {'index': 10, 'occupation': 0.0},\n",
      "           {'index': 11, 'occupation': 0.0}],\n",
      " 'gpus': [{'index': 0, 'occupation': 0.0}],\n",
      " 'index': 0,\n",
      " 'lfs': 1000000,\n",
      " 'mem': 65536,\n",
      " 'name': 'localhost'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(node.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also register our manually created slot as occupied so that later calls to `find_slot` will take that information into account (after use, a slot should be deallocated again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cores': [{'index': 0, 'occupation': 1.0},\n",
      "           {'index': 1, 'occupation': 0.0},\n",
      "           {'index': 2, 'occupation': 0.0},\n",
      "           {'index': 3, 'occupation': 1.0},\n",
      "           {'index': 4, 'occupation': 0.0},\n",
      "           {'index': 5, 'occupation': 0.0},\n",
      "           {'index': 6, 'occupation': 0.0},\n",
      "           {'index': 7, 'occupation': 0.0},\n",
      "           {'index': 8, 'occupation': 0.0},\n",
      "           {'index': 9, 'occupation': 0.0},\n",
      "           {'index': 10, 'occupation': 0.0},\n",
      "           {'index': 11, 'occupation': 0.0}],\n",
      " 'gpus': [{'index': 0, 'occupation': 0.0}],\n",
      " 'index': 0,\n",
      " 'lfs': 1000000,\n",
      " 'mem': 65536,\n",
      " 'name': 'localhost'}\n"
     ]
    }
   ],
   "source": [
    "node.allocate_slot(slot_1)\n",
    "pprint.pprint(node.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allocate a _set_ of slots, for example for a multi-rank task, RP can search the node list itself for available resources.  That search might return slots which are distributed across all nodes.  For example, the call below will allocate the resources for 4 ranks where each rank uses 4 cores and half a GPU (2 ranks can share one GPU).  As the GPU is the limiting resource in this scenario, we will be able to place at most 2 ranks per node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Slot: {'cores': [RO: 1:1.00, RO: 2:1.00, RO: 4:1.00, RO: 5:1.00], 'gpus': [RO: 0:0.50], 'lfs': 0, 'mem': 0, 'node_index': 0, 'node_name': 'localhost', 'version': 1},\n",
      " Slot: {'cores': [RO: 6:1.00, RO: 7:1.00, RO: 8:1.00, RO: 9:1.00], 'gpus': [RO: 0:0.50], 'lfs': 0, 'mem': 0, 'node_index': 0, 'node_name': 'localhost', 'version': 1},\n",
      " Slot: {'cores': [RO: 0:1.00, RO: 1:1.00, RO: 2:1.00, RO: 3:1.00], 'gpus': [RO: 0:0.50], 'lfs': 0, 'mem': 0, 'node_index': 1, 'node_name': 'localhost', 'version': 1},\n",
      " Slot: {'cores': [RO: 4:1.00, RO: 5:1.00, RO: 6:1.00, RO: 7:1.00], 'gpus': [RO: 0:0.50], 'lfs': 0, 'mem': 0, 'node_index': 1, 'node_name': 'localhost', 'version': 1}]\n"
     ]
    }
   ],
   "source": [
    "rr = rp.RankRequirements(n_cores=4, n_gpus=1, gpu_occupation=0.5)\n",
    "slots = nodelist.find_slots(rr, n_slots=4)\n",
    "pprint.pprint(slots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Level Scheduling\n",
    "\n",
    "With the above tools, the simplest implementation of an application level scheduler would be:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "submit: \u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m#\u001b[39m\u001b[0m\n",
      "\u001b[39m\u001b[0m"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tasks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 74\u001b[0m\n\u001b[1;32m     67\u001b[0m     td \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexecutable\u001b[39m\u001b[38;5;124m'\u001b[39m     : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msleep\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     68\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marguments\u001b[39m\u001b[38;5;124m'\u001b[39m      : [i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m5\u001b[39m],\n\u001b[1;32m     69\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mranks\u001b[39m\u001b[38;5;124m'\u001b[39m          : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     70\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcores_per_rank\u001b[39m\u001b[38;5;124m'\u001b[39m : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     71\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslots\u001b[39m\u001b[38;5;124m'\u001b[39m          : slots}\n\u001b[1;32m     72\u001b[0m     tds\u001b[38;5;241m.\u001b[39mappend(rp\u001b[38;5;241m.\u001b[39mTaskDescription(td))\n\u001b[0;32m---> 74\u001b[0m \u001b[43mrun_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpilot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodelist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m, in \u001b[0;36mrun_tasks\u001b[0;34m(tmgr, nodelist, tds)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m submitted:\n\u001b[1;32m     32\u001b[0m     running_tasks[task\u001b[38;5;241m.\u001b[39muid] \u001b[38;5;241m=\u001b[39m task\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mtasks\u001b[49m[task\u001b[38;5;241m.\u001b[39muid] \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmitted \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m task\u001b[38;5;241m.\u001b[39muid)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m not_allocated:\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# all tasks are submitted - wait for completion\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tasks' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_tasks(tmgr, nodelist, tds):\n",
    "    '''\n",
    "    tmgr    : task manager which handles execution of tasks\n",
    "    nodelist: resources to be used for task execution\n",
    "    tds     : list of rp.TaskDescriptions - list of tasks to run\n",
    "    '''\n",
    "\n",
    "    running_tasks   = dict()\n",
    "    completed_tasks = list()\n",
    "    \n",
    "    while tds:\n",
    "\n",
    "        # find slots for all task descriptions\n",
    "        allocated = list()\n",
    "        not_allocated = list()\n",
    "\n",
    "        for td in tds:\n",
    "            rr = rp.RankRequirements(n_cores=td.cores_per_rank, n_gpus=td.gpus_per_rank, \n",
    "                                     mem=td.mem_per_rank,  lfs=td.lfs_per_rank)\n",
    "            slots = nodelist.find_slots(rr, n_slots=td.ranks)\n",
    "            if slots:\n",
    "                # this task can be submitted\n",
    "                td.slots = slots\n",
    "                allocated.append(td)\n",
    "            else:\n",
    "                # this task has to be retries later on\n",
    "                not_allocated.append(td)\n",
    "\n",
    "        # submit all tasks for which resources were found\n",
    "        submitted = tmgr.submit_tasks(allocated)\n",
    "        for task in submitted:\n",
    "            running_tasks[task.uid] = task\n",
    "            tasks[task.uid] = task\n",
    "            print('submitted %d' % task.uid)\n",
    "\n",
    "        if not not_allocated:\n",
    "\n",
    "            # all tasks are submitted - wait for completion\n",
    "            tmgr.wait_tasks()\n",
    "            break\n",
    "\n",
    "        else:\n",
    "\n",
    "            # could not submit all tasks - wait for resources to become available \n",
    "            # on and attempt to schedule the remaining tasks in the next iteration\n",
    "            tds = not_allocated\n",
    "            \n",
    "            while True:\n",
    "                uids   = list(running_tasks.keys())\n",
    "                states = tmgr.wait_tasks(state=rp.FINAL, uids=uids, timeout=5.0)\n",
    "                                \n",
    "                # if some task completed, we should have new resources\n",
    "                resources = False\n",
    "                for uid, state in zip(uids, states):\n",
    "                    if state in rp.FINAL:\n",
    "                        compleded_tasks.append(running_tasks[uid])\n",
    "                        pilot.nodelist.release_slots(task.description.slots)\n",
    "                        del running_tasks[uid]\n",
    "                        resources = True\n",
    "\n",
    "                # if we got any free resources, try to schedule more tasks\n",
    "                if resources:\n",
    "                    break\n",
    "\n",
    "tds = list()\n",
    "for i in range(5):\n",
    "    td = {'executable'     : 'sleep', \n",
    "          'arguments'      : [i + 5],\n",
    "          'ranks'          : i + 1, \n",
    "          'cores_per_rank' : i + 1,\n",
    "          'slots'          : slots}\n",
    "    tds.append(rp.TaskDescription(td))\n",
    "\n",
    "run_tasks(tmgr, pilot.nodelist, tds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
