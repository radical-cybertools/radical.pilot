{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration System\n",
    "\n",
    "RADICAL-Pilot (RP) uses a configuration system to set control and management parameters for the initialization of its components and to define resource entry points for the target platform.\n",
    "\n",
    "It includes:\n",
    "\n",
    "* [Run description](#Run-description)\n",
    "  * Resource label for a target platform configuration file;\n",
    "  * Project allocation name (i.e., account/project, _specific for HPC platforms_);\n",
    "  * Job queue name (i.e., queue/partition, _specific for HPC platforms_);\n",
    "  * Amount of the resources (e.g., `cores`, `gpus`, `memory`) to allocate for the runtime period;\n",
    "  * Mode to access the target platform (e.g., `local`, `ssh`, `batch/interactive`).\n",
    "* [Target platform description](#Platform-description)\n",
    "  * Batch system (e.g., `SLURM`, `LSF`, etc.);\n",
    "  * Provided launch methods (e.g., `SRUN`, `MPIRUN`, etc.);\n",
    "  * Environment setup (including package manager, working directory, etc.);\n",
    "  * Entry points: batch system URL, file system URL.\n",
    "\n",
    "## Run description\n",
    "\n",
    "Users have to describe at least one pilot in each RP application. That is done by instantiating a [radical.pilot.PilotDescription](../apidoc.rst#PilotDescription) object. Among that object's attributes, `resource` is mandatory and is referred as a resource label (or platform ID), which corresponds to a target platform configuration file (see the section [Platform description](#Platform-description)). Users need to know what ID corresponds to the HPC platform on which they want to execute their RP application.\n",
    "\n",
    "### Allocation parameters\n",
    "\n",
    "Every run should state the project name (i.e., allocation account), preferable queue for a job submission, and the amount of required resources explicitly, unless it is a local run without accessing any batch system.\n",
    "\n",
    "```python\n",
    "import radical.pilot as rp\n",
    "\n",
    "pd = rp.PilotDescription({\n",
    "    'resource': 'ornl.frontier',  # platform ID\n",
    "    'project' : 'XYZ000',         # allocation account\n",
    "    'queue'   : 'debug',          # optional (default value might be set in the platform description)\n",
    "    'cores'   : 32,               # amount of CPU slots\n",
    "    'gpus'    : 8,                # amount of GPU slots\n",
    "    'runtime' : 15                # maximum runtime for a pilot (in minutes)\n",
    "})\n",
    "```\n",
    "\n",
    "### Resource access schema\n",
    "\n",
    "Resource access schema (`pd.access_schema`) is provided as part of a platform description, and in case of more than one access schemas users can set a specific one in [radical.pilot.PilotDescription](../apidoc.rst#PilotDescription). Check schema availability per target platform:\n",
    "\n",
    "* `local`: launch RP application from the target platform (e.g., login nodes of the specific machine).\n",
    "* `ssh`: launch RP application outside the target platform and use `ssh` protocol and corresponding SSH client to access the platform remotely.\n",
    "* `gsissh`: launch RP application outside the target platform and use GSI-enabled SSH to access the platform remotely.\n",
    "* `interactive`: launch RP application from the target platform within the interactive session after being placed on allocated resources (e.g., batch or compute nodes).\n",
    "* `batch`: launch RP application by a submitted batch script at the target platform.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__Note:__ For details on submission of applications on HPC see the tutorial [Using RADICAL-Pilot on HPC Platforms](submission.ipynb).\n",
    "\n",
    "</div>\n",
    "\n",
    "## Platform description\n",
    "\n",
    "The RADICAL-Pilot uses configuration files for bookkeeping of supported platforms. Each configuration file identifies a facility (e.g., ACCESS, TACC, ORNL, ANL, etc.), is written in JSON and is named following the `resource_<facility_name>.json` convention. Each facility configuration file contains a set of platform names/labels with corresponding configuration parameters. Resource label (or platform ID) follows the `<facility_name>.<platform_name>` convention, and users use it for the `resource` attribute of their [radical.pilot.PilotDescription](../apidoc.rst#PilotDescription) object.\n",
    "\n",
    "### Predefined configurations\n",
    "\n",
    "The RADICAL-Pilot development team maintains a growing set of pre-defined configuration files for supported HPC platforms (list platform descriptions in RP's [GitHub repo](https://github.com/radical-cybertools/radical.pilot/tree/master/src/radical/pilot/configs)).\n",
    "\n",
    "For example, if users want to execute their RP application on Frontera, they will have to search for the [resource_tacc.json](https://github.com/radical-cybertools/radical.pilot/blob/master/src/radical/pilot/configs/resource_tacc.json) file and, inside that file, for the key(s) that start with the name `frontera`. The file `resource_tacc.json` contains the keys `frontera`, `frontera_rtx`, and `frontera_prte`. Each key identifies a specific set of configuration parameters: `frontera` offers a general-purpose set of configuration parameters; `frontera_rtx` enables the use of the `rtx` queue for GPU nodes; and `frontera_prte` enables the use of the PRTE-based launch method to execute the application's tasks. Thus, for Frontera, the value for `resource` will be `tacc.frontera`, `tacc.frontera_rtx` or `tacc.frontera_prte`.  \n",
    "\n",
    "### Customizing a predefined configuration\n",
    "\n",
    "Users can customize existing platform configuration files by overwriting existing key/value pairs with ones from configuration files, which have the same names, but located in a user space. Default location of user-defined configuration files is `$HOME/.radical/pilot/configs/`. \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__Note:__ To change the location for user-defined platform configuration files, please, use env variable `RADICAL_CONFIG_USER_DIR`, which will be used instead of env variable `HOME` in the location path above. Make sure that the corresponding path exists, before creating configs there.\n",
    "\n",
    "</div>\n",
    "\n",
    "Two examples of customized configurations are below: (i) in one for `ornl.summit` you change parameter __system_architecture.options__, and (ii) in another for `tacc.frontera` you set a default launch method `MPIEXEC`. With that files, every pilot description using `'resource': 'ornl.summit'` or `'resource': 'tacc.frontera'` would use that new values. Changed parameters are described in the following section.\n",
    "\n",
    "__resource_ornl.json__\n",
    "```json\n",
    "{\n",
    "    \"summit\": {\n",
    "        \"system_architecture\": {\n",
    "            \"options\": [\"gpumps\", \"gpudefault\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "__resource_tacc.json__\n",
    "```json\n",
    "{\n",
    "    \"frontera\": {\n",
    "        \"launch_methods\": {\n",
    "            \"order\"  : [\"MPIEXEC\"],\n",
    "            \"MPIEXEC\": {}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### User-defined configuration\n",
    "\n",
    "Users can write whole new configuration for an existing or a new platform with arbitrary platform ID. For example, you will create a custom platform configuration entry `resource_tacc.json` locally. That file will be loaded into the RP's [radical.pilot.Session](../apidoc.rst#Sessions) object alongside with other configurations for TACC-related platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_tacc_tutorial = \\\n",
    "{\n",
    "    \"frontera_tutorial\":\n",
    "    {\n",
    "        \"description\"                 : \"Short description of the resource\",\n",
    "        \"notes\"                       : \"Notes about resource usage\",\n",
    "\n",
    "        \"default_schema\"              : \"local\",\n",
    "        \"schemas\"                     : {\n",
    "            \"local\"                   : {\n",
    "                \"job_manager_endpoint\": \"slurm://frontera.tacc.utexas.edu/\",\n",
    "                \"filesystem_endpoint\" : \"file://frontera.tacc.utexas.edu/\"\n",
    "            },\n",
    "            \"ssh\"                     : {\n",
    "                \"job_manager_endpoint\": \"slurm+ssh://frontera.tacc.utexas.edu/\",\n",
    "                \"filesystem_endpoint\" : \"sftp://frontera.tacc.utexas.edu/\"\n",
    "            },\n",
    "            \"batch\"                   : \"interactive\",\n",
    "            \"interactive\"             : {\n",
    "                \"job_manager_endpoint\": \"fork://localhost/\",\n",
    "                \"filesystem_endpoint\" : \"file://localhost/\"\n",
    "            },\n",
    "        },\n",
    "\n",
    "        \"default_queue\"               : \"production\",\n",
    "        \"resource_manager\"            : \"SLURM\",\n",
    "\n",
    "        \"cores_per_node\"              : 56,\n",
    "        \"gpus_per_node\"               : 0,\n",
    "        \"system_architecture\"         : {\n",
    "                                         \"smt\"           : 1,\n",
    "                                         \"options\"       : [\"nvme\", \"intel\"],\n",
    "                                         \"blocked_cores\" : [],\n",
    "                                         \"blocked_gpus\"  : []\n",
    "                                        },\n",
    "\n",
    "        \"agent_config\"                : \"default\",\n",
    "        \"agent_scheduler\"             : \"CONTINUOUS\",\n",
    "        \"agent_spawner\"               : \"POPEN\",\n",
    "        \"default_remote_workdir\"      : \"$HOME\",\n",
    "\n",
    "        \"pre_bootstrap_0\"             : [\n",
    "                                        \"module unload intel impi\",\n",
    "                                        \"module load   intel impi\",\n",
    "                                        \"module load   python3/3.9.2\"\n",
    "                                        ],\n",
    "        \"launch_methods\"              : {\n",
    "                                         \"order\"  : [\"MPIRUN\"],\n",
    "                                         \"MPIRUN\" : {\n",
    "                                             \"pre_exec_cached\": [\n",
    "                                                 \"module load TACC\"\n",
    "                                             ]\n",
    "                                         }\n",
    "                                        },\n",
    "        \n",
    "        \"python_dist\"                 : \"default\",\n",
    "        \"virtenv_mode\"                : \"local\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definition of each field:\n",
    "\n",
    "* __description__ (_optional_) - human-readable description of the platform.\n",
    "* __notes__ (_optional_) - information needed to form valid pilot descriptions, such as what parameters are required, etc.\n",
    "* __schemas__ - allowed values for the `pd.access_schema` attribute of the pilot description. The first schema in the list is used by default. For each schema, a subsection is needed, which specifies __job_manager_endpoint__ and __filesystem_endpoint__.\n",
    "* __job_manager_endpoint__ - access URL for pilot submission (interpreted by RADICAL-SAGA).\n",
    "* __filesystem_endpoint__ - access URL for file staging (interpreted by RADICAL-SAGA).\n",
    "* __default_queue__ (_optional_) - queue name to be used for pilot submission to a corresponding batch system (see __job_manager_endpoint__).\n",
    "* __resource_manager__ - the type of job management system. Valid values are: `CCM`, `COBALT`, `FORK`, `LSF`, `PBSPRO`, `SLURM`, `TORQUE`, `YARN`.\n",
    "* __cores_per_node__ (_optional_) - number of available CPU cores per compute node. If not provided then it will be discovered by RADICAL-SAGA and by Resource Manager in RADICAL-Pilot.\n",
    "* __gpus_per_node__ (_optional_) - number of available GPUs per compute node. If not provided then it will be discovered by RADICAL-SAGA and by Resource Manager in RADICAL-Pilot.\n",
    "* __system_architecture__ (_optional_) - set of options that describe platform features:\n",
    "   * __smt__ - Simultaneous MultiThreading (i.e., threads per physical core). If it is not provided then the default value `1` is used. It could be reset with env variable `RADICAL_SMT` exported before running RADICAL-Pilot application. RADICAL-Pilot uses `cores_per_node x smt` to calculate all available cores/CPUs per node.\n",
    "   * __options__ - list of job management system specific attributes/constraints, which are provided to RADICAL-SAGA.\n",
    "      * `COBALT` uses option `--attrs` for configuring location as `filesystems=home,grand`, `mcdram` as `mcdram=flat`, `numa` as `numa=quad`;\n",
    "      * `LSF` uses option `-alloc_flags` to support `gpumps`, `nvme`;\n",
    "      * `PBSPRO` uses option `-l` for configuring location as `filesystems=grand:home`, placement as `place=scatter`;\n",
    "      * `SLURM` uses option `--constraint` for compute nodes filtering.\n",
    "   * __blocked_cores__ - list of cores/CPUs indices, which are not used by Scheduler in RADICAL-Pilot for tasks assignment.\n",
    "   * __blocked_gpus__ - list of GPUs indices, which are not used by Scheduler in RADICAL-Pilot for tasks assignment.\n",
    "* __agent_config__ - configuration file for RADICAL-Pilot Agent (default value is `default` for a corresponding file [agent_default.json](https://github.com/radical-cybertools/radical.pilot/blob/master/src/radical/pilot/configs/agent_default.json)).\n",
    "* __agent_scheduler__ - Scheduler in RADICAL-Pilot (default value is `CONTINUOUS`).\n",
    "* __agent_spawner__ - Executor in RADICAL-Pilot, which spawns task execution processes (default value is `POPEN`).\n",
    "* __default_remote_workdir__ (_optional_) - directory for agent sandbox (see the tutorials [Getting Started](../getting_started.ipynb#Generated-Output) and [Staging Data with RADICAL-Pilot](staging_data.ipynb#Locations)). If not provided then the current directory is used (`$PWD`).\n",
    "* __forward_tunnel_endpoint__ (_optional_) - name of the host, which can be used to create ssh tunnels from the compute nodes to the outside of the platform.\n",
    "* __pre_bootstrap_0__ (_optional_) - list of commands to execute for the bootstrapping process to launch RADICAL-Pilot Agent.\n",
    "* __pre_bootstrap_1__ (_optional_) - list of commands to execute for initialization of sub-agent, which are used to run additional instances of RADICAL-Pilot components such as Executor and Stager.\n",
    "* __launch_methods__ - set of supported launch methods. Valid values are `APRUN`, `CCMRUN`, `FLUX`, `FORK`, `IBRUN`, `JSRUN` (`JSRUN_ERF`), `MPIEXEC` (`MPIEXEC_MPT`), `MPIRUN` (`MPIRUN_CCMRUN`, `MPIRUN_DPLACE`, `MPIRUN_MPT`, `MPIRUN_RSH`), `PRTE`, `RSH`, `SRUN`, `SSH`. For each launch method, a subsection is needed, which specifies __pre_exec_cached__ with list of commands to be executed to configure the launch method, and method related options (e.g., __dvm_count__ for `PRTE`).\n",
    "   * __order__ - sets the order of launch methods to be selected for the task placement (the first value in the list is a default launch method).\n",
    "* __python_dist__ - python distribution. Valid values are `default` and `anaconda`.\n",
    "* __virtenv_mode__ - bootstrapping process set the environment for RADICAL-Pilot Agent:\n",
    "   * `create` - create a python virtual environment from scratch;\n",
    "   * `recreate` - delete the existing virtual environment and build it from scratch, if not found then `create`;\n",
    "   * `use` - use the existing virtual environment, if not found then `create`;\n",
    "   * `update` - update the existing virtual environment, if not found then `create` (_default_);\n",
    "   * `local` - use the client existing virtual environment (environment from where RADICAL-Pilot application was launched).\n",
    "* __virtenv__ (_optional_) - path to the existing virtual environment or its name with the pre-installed RCT stack; use it only when `virtenv_mode=use`.\n",
    "* __rp_version__ - RADICAL-Pilot installation or reuse process:\n",
    "   * `local` - install from tarballs, from client existing environment (_default_);\n",
    "   * `release` - install the latest released version from PyPI;\n",
    "   * `installed` - do not install, target virtual environment has it.\n",
    "\n",
    "## Examples\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "__Note:__ In our examples, we will not show a progression bar while waiting for some operation to complete, e.g., while waiting for a pilot to stop. That is because the progression bar offered by RP's reporter does not work within a notebook. You could use it when executing an RP application as a standalone Python script.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env RADICAL_REPORT_ANIME=FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the location for user-defined configurations exists\n",
    "!mkdir -p \"${RADICAL_CONFIG_USER_DIR:-$HOME}/.radical/pilot/configs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import radical.pilot as rp\n",
    "import radical.utils as ru"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the next steps, you will save the earlier created configuration for a target platform into the file `resource_tacc.json`, located in a user-space. You also will be able to read that file and print some of its attributes to confirm that they are in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save earlier defined platform configuration into the user-space\n",
    "ru.write_json(resource_tacc_tutorial, os.path.join(os.path.expanduser('~'), '.radical/pilot/configs/resource_tacc.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial_cfg = rp.utils.get_resource_config(resource='tacc.frontera_tutorial')\n",
    "\n",
    "for attr in ['schemas', 'resource_manager', 'cores_per_node', 'system_architecture']:\n",
    "    print('%-20s : %s' % (attr, tutorial_cfg[attr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('job_manager_endpoint : ', rp.utils.get_resource_job_url(resource='tacc.frontera_tutorial', schema='ssh'))\n",
    "print('filesystem_endpoint  : ', rp.utils.get_resource_fs_url (resource='tacc.frontera_tutorial', schema='ssh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = rp.Session()\n",
    "pmgr    = rp.PilotManager(session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial_cfg = session.get_resource_config(resource='tacc.frontera_tutorial', schema='batch')\n",
    "for attr in ['label', 'launch_methods', 'job_manager_endpoint', 'filesystem_endpoint']:\n",
    "    print('%-20s : %s' % (attr, ru.as_dict(tutorial_cfg[attr])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Platform description created above is also available within the [radical.pilot.Session](../apidoc.rst#Sessions) object. Let's confirm that newly created resource description is within the session. `Session` object has all provided platform configurations (pre- and user-defined ones), thus for a pilot you just need to select a particular configuration and a corresponding access schema (as part of the pilot description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = rp.PilotDescription({\n",
    "    'resource'     : 'tacc.frontera_tutorial',\n",
    "    'project'      : 'XYZ000',\n",
    "    'queue'        : 'production',\n",
    "    'cores'        : 56,\n",
    "    'runtime'      : 15,\n",
    "    'access_schema': 'batch',\n",
    "    'exit_on_error': False\n",
    "})\n",
    "\n",
    "pilot = pmgr.submit_pilots(pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(pilot.as_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring pilot setup and configuration we close the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close(cleanup=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
