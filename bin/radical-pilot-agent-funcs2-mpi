#!/usr/bin/env python3

import os
import dill
import time
import codecs
import pickle
import msgpack
from mpi4py import MPI

import radical.utils as ru

IDLE = False
BUSY = True

MPI.pickle.__init__(dill.dumps, dill.loads)


# ------------------------------------------------------------------------------
#
class MPIExecutor(object):
    '''
    This class executes MPI functions in a new, private sub-communicator.

    The class should be used as follows:

        executor = MPIExecutor()
        executor.work()

    The code above should run in *all* ranks of `MPI_COMM_WORLD`, i.e., the code
    above should run under (for example):

        mpirun -n 16 --use-hwthread-cpus executor.py

    The class will run `self.work_master()` on rank 0, and `self.work_worker()`
    on all other ranks.

    Master:
    -------

    The master then iterates over incoming tasks (well, over a static task list
    at the moment) and will try to find as many idle workers as the task
    requires (`task['ranks']`).  It then sends a direct MPI message (tag `0`) to
    exactly those workers.

    Once all tasks are sent, the master will collect all task results (`recv`
    with message tag `1`).  Once all results are received, the master will send
    a termination message to all workers (tag `0`, message `None`), and will
    terminate -- as will the workers when receiving that message.


    Worker:
    -------

    The worker will wait for an incoming message with tag `0` (task to run).
    From the received task it will determine with what other workers it should
    create a  sub-communicator.  The respective process group is created as is
    the new communicator, and the workload (the methods `self.test_1` or
    `self.test_2`) are called, passing the sub-communicator as first argument.
    The methods can then use the communicator to communicate, and will
    eventually return.  Whatever value rank `0` of that sub-communicator will
    return is stored as result of the task, and the worker of that rank `0` will
    send the task back to the master (tag `1`).

    If the worker receives a `None` message, it will terminate.
    '''

    # --------------------------------------------------------------------------
    #
    def __init__(self):

        # ensure we have a communicator
        self._world = MPI.COMM_WORLD
        self._group = self._world.Get_group()
        self._pwd   = os.getcwd()
        self._uid   = os.environ.get('RP_FUNCS_ID')
        self._log   = ru.Logger(self._uid,   ns='radical.pilot', path=self._pwd)
        self._prof  = ru.Profiler(self._uid, ns='radical.pilot', path=self._pwd)

    # --------------------------------------------------------------------------
    #
    @property
    def rank(self) -> int:
        return self._world.rank

    @property
    def size(self) -> int:
        return self._world.size

    @property
    def master(self) -> bool:
        return bool(self.rank == 0)

    @property
    def worker(self) -> bool:
        return bool(self.rank != 0)


    # --------------------------------------------------------------------------
    #
    def _out(self, msg):
        '''
        small helper for debug output
        '''

        if self.master: self._log.debug('=== %3d: %s' % (self.rank, msg))
        else          : self._log.debug('  - %3d: %s' % (self.rank, msg))


    # --------------------------------------------------------------------------
    #
    def work(self):
        '''
        master (rank == 0) does master stuff
        worker (rank != 0) does worker stuff
        '''

        if self.master: self.work_master()
        else          : self.work_worker()


    # --------------------------------------------------------------------------
    #
    def work_master(self):
        # keep track of busy and idle workers (not really used, yet)
        resources = [IDLE] * (self.size - 1)

        # define a static list of tasks
        #    uid  : duh!
        #    ranks: how many workers to use in the task's communicator
        #    work : what method to call
        #    args : what arguments to pass to the method

        if self.rank == 0:

            _cfg      = ru.read_json('%s/%s.cfg' % (self._pwd, self._uid))

            addr_req  = _cfg.get('req_get')
            addr_res  = _cfg.get('res_put')
            addr_ctrl = _cfg.get('ctrl')

            assert(addr_req)
            assert(addr_res)
            assert(addr_ctrl)


            _zmq_req  = ru.zmq.Getter(channel='funcs_req_queue', url=addr_req)
            _zmq_res  = ru.zmq.Putter(channel='funcs_res_queue', url=addr_res)
            _zmq_ctrl = ru.zmq.Getter(channel='control_pubsub',  url=addr_ctrl)


        while True:
            if self.rank == 0:
                tasks = _zmq_req.get_nowait(1000)

                if tasks:

                    for task in tasks:
                        self._log.debug('got %s', task['uid'])
                        task_descr = task['description']
                        task_exe   = task_descr['executable']
                        task_args  = task_descr.get('arguments', list())
                        task_pres  = task_descr.get('pre_exec',  list())
                        task_ranks = task_descr['cpu_processes']

                        try:

                            task_info = pickle.loads(codecs.decode(task_exe.encode(),"base64"))

                            task['work']  = task_info["_cud_code"]
                            task['args']  = task_info["_cud_args"]
                            task['ranks'] = task_ranks

                        except Exception as e:
                            self._log.debug('ERROR %s', str(e))

                        # find workers and mark as busy
                        workers = list()
                        for idx, worker in enumerate(resources):
                            if worker == IDLE:
                                workers.append(idx + 1)
                                resources[idx] = BUSY
                                if len(workers) == task_ranks:
                                    self._log.debug('enough workers found')
                                    break

                        # make sure we did find workers!
                        assert(len(workers) == task_ranks)


                        # workers need to know who is part of the sub-communicator
                        task['workers'] = workers

                        # send the task to each of the workers
                        self._log.debug('send task %s  to  %s' % (task['uid'], workers))
                        for worker in workers:
                            self._world.send(msgpack.packb(task), dest=worker, tag=0)
                        
                        # FIXME: Now release the BUSY workers and mark them as IDLE to be 
                        # resuesd by the next task 
                        for worker in workers:
                            if worker == BUSY:
                                workers.append(idx + 1)
                                resources[idx] = IDLE
                                
                                self._log.debug('IDLE WOKERS %d' % (worker))
                            else:
                                self._log.debug('BUSY WOKERS %d' % (worker))
                                pass

                    # once all tasks are sent out, collect results
                    for task in tasks:
                        task = msgpack.unpackb(self._world.recv(tag=1))
                        if  task['state'] == 'DONE':
                            task['stdout'] = task['result']
                            task['stderr'] = None
                            task['work']   = str(task['work'])

                        else:
                            task['stdout'] = None
                            task['stderr'] = task['error']
                            task['work']   = str(task['work'])

                        _zmq_res.put(task)

                    # once all results are received, send termination signal to workers
                    for worker in range(self.size - 1):
                        self._log.debug('send term to %s' % (worker + 1))
                        self._world.send(msgpack.packb(None), dest=worker + 1, tag=0)



    # --------------------------------------------------------------------------
    #
    def work_worker(self):

        # wait for termination message
        while True:

            task = msgpack.unpackb(self._world.recv(source=0, tag=0))

            if not task:
                self._log.debug('terminate!')
                break

            self._log.debug('recv task %s from Master >>>' % (task['uid']))
            assert(self.rank in task['workers'])

            comm  = None
            group = None

            try:
                # create new communicator with all workers assigned to this task
                group = self._group.Incl(task['workers'])
                comm  = self._world.Create_group(group)
                assert(comm)

                # work on task
                fn = MPI.pickle.loads(task['work'])
                result = fn(*task['args'])

                # result is only reported back by rank 0 (of sub-communicator)
                if comm.rank == 0:
                    task['result'] = result
                    task['state']  = "DONE"
                    self._log.debug('send res  %s to Master <<< ' % (task['uid']))
                    self._world.send(msgpack.packb(task), dest=0, tag=1)

            except Exception as e:
                if comm.rank == 0:
                    task['error']  = str(e)
                    task['state']  = "FAILED"
                    task['result'] = None

                self._log.error('recv err %s to Master' % (task['uid']))
                self._log.error(str(e))
                self._world.send(msgpack.packb(task), dest=0, tag=1)
                raise

            finally:
                # sub-communicator must always be destroyed
                if group: group.Free()
                if comm : comm.Free()


# ------------------------------------------------------------------------------
#
if __name__ == '__main__':

    executor = MPIExecutor()
    executor.work()


# ------------------------------------------------------------------------------
