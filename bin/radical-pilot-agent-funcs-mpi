#!/usr/bin/env python

import os
import sys
import time
import dill
import queue
import pickle
import codecs
import itertools

import multiprocessing as mp
import threading       as mt

import radical.utils   as ru
from radical.pilot.agent import MPI_Func_Worker

pwd = sys.argv[1]

# ------------------------------------------------------------------------------
# activate virtenv if needed
ve = None
if len(sys.argv) > 2:
    ve = sys.argv[2]

if ve and ve not in ['', 'None', None]:

    activate = "%s/bin/activate_this.py" % ve

    exec(open(activate).read(), dict(__file__=activate))

# ------------------------------------------------------------------------------
#
class Tasker():
    '''
    This executor is running as an RP task (master) and owns a complete node.
    On each core of that node and other nodes (if possible), it spawns a specail 
    mpi-worker(s) process to execute function calls.Communication to those processes
    is establshed via two mp.Queue instances, one for feeding call requests to the 
    worker processes, and one to collect results from their execution.

    Once the mpi-workers are prepared, the Executor will listens on an task level
    ZMQ channel for incoming call requests, which are then proxied to the
    workers as described above.  This happens in a separate thread.  Another
    thread is spawned to inversely collect the results as described above and to
    proxy them to an outgoing ZMQ channel.  The Executor main thread will listen
    on a 3rd ZMQ channel for control messages, and specifically for termination
    commands.

    This class takes an executor (currently we set it up to MPI executot) and check
    if the executor is provided it will use that executor to run the tasks (assuming that
    these tasks are valid tasks for the provided executors).
    '''

    # --------------------------------------------------------------------------
    #
    def __init__(self, n_workers=None):
        
        '''
        Since it is possible, we might need to provide the n_workers to:
        MPIExecutor(max_workers = n_workers)
        '''
        self._nw       = n_workers
        self._uid      = os.environ['RP_FUNCS_ID']
        self._log      = ru.Logger(self._uid,   ns='radical.pilot', path=pwd)
        self._prof     = ru.Profiler(self._uid, ns='radical.pilot', path=pwd)
        self._cfg      = ru.read_json('%s/%s.cfg' % (pwd, self._uid))
        self._worker   = MPI_Func_Worker()

        self._initialize()


    # --------------------------------------------------------------------------
    #
    def _initialize(self):
        '''
        set up processes, threads and communication channels
        '''

        self._prof.prof('init_start', uid=self._uid)

        addr_req  = self._cfg.get('req_get')
        addr_res  = self._cfg.get('res_put')
        addr_ctrl = self._cfg.get('ctrl')

        self._log.debug('req get addr: %s', addr_req)
        self._log.debug('res put addr: %s', addr_res)
        self._log.debug('ctrl    addr: %s', addr_ctrl)

        assert(addr_req)
        assert(addr_res)
        assert(addr_ctrl)

        # connect to
        #
        #   - the queue which feeds us tasks
        #   - the queue were we send completed tasks
        #   - the command queue (for termination)
        #
        self._zmq_req  = ru.zmq.Getter(channel='funcs_req_queue', url=addr_req)
        self._zmq_res  = ru.zmq.Putter(channel='funcs_res_queue', url=addr_res)
        self._zmq_ctrl = ru.zmq.Getter(channel='control_pubsub',  url=addr_ctrl)

        # use mp.Queue instances to proxy tasks to the worker processes
        self._mpq_work    = mp.Queue()
        self._mpq_result  = mp.Queue()

        # signal for thread termination
        self._term = mt.Event()

        # start threads to feed / drain the workers
        self._t_get_work    = mt.Thread(target=self._get_work)
        self._t_get_results = mt.Thread(target=self._get_results)

        self._t_get_work.daemon    = True
        self._t_get_results.daemon = True

        self._t_get_work.start()
        self._t_get_results.start()

        wid  = '%s.%03d' % (self._uid, 1)
    
        self._watcher = mt.Thread(target=self.watch)
        self._watcher.daemon = True
        self._watcher.start()
    
        # start the work
        self._work(self._uid, wid)
        self._prof.prof('init_stop', uid=self._uid)


    # --------------------------------------------------------------------------
    #
    def watch(self):
        '''
        listen on the command channel for things to do (like, terminate).
        '''

        try:
            while True:

                msgs = self._zmq_ctrl.get_nowait(100)
                time.sleep(1)

                if not msgs:
                    continue

                for msg in msgs:

                    self._prof.prof('cmd', uid=self._uid, msg=msg['cmd'])

                    if msg['cmd'] == 'term':

                        sys.exit(0)

                    else:
                        self._log.error('unknown command %s', msg)

        finally:

            # kill worker processes
            # The MPIExecutor will handle the termination so 
            pass


    # --------------------------------------------------------------------------
    #
    def _get_work(self):
        '''
        thread feeding tasks pulled from the ZMQ work queue to worker processes
        '''

        # FIXME: This drains the qork queue with no regard of load balancing.
        #        For example, the first <n_cores> tasks may stall this executer
        #        for a long time, but new tasks are pulled nonetheless, even if
        #        other executors are not stalling and could execute them timely.
        #        We should at most fill a cache of limited size.

        while not self._term.is_set():

            tasks = self._zmq_req.get_nowait(1000)

            if tasks:

                self._log.debug('got %d tasks', len(tasks))

                # send task individually to load balance workers
                for task in tasks:
                    self._mpq_work.put(task)


    # --------------------------------------------------------------------------
    #
    def _get_results(self):
        '''
        thread feeding back results from to workers to the result ZMQ queue
        '''

        while not self._term.is_set():

            # we always pull *individual* tasks from the result queue
            try:
                task = self._mpq_result.get(block=True, timeout=0.1)

            except queue.Empty:
                continue

            if task:
                self._zmq_res.put(task)
    # --------------------------------------------------------------------------
    #
    def _work(self, uid, wid):
        '''
        work loop for worker processes: pull a task from the work queue,
        run it, push the result onto the result queue
        '''

        self._prof.prof('work_start', comp=wid, uid=uid)

        while True:

            try:
                task = self._mpq_work.get(block=True, timeout=0.1)

            except queue.Empty:
                continue

            tid   = task['uid']
            descr = task['description']
            exe   = descr['executable']
            args  = descr.get('arguments', list())
            pres  = descr.get('pre_exec',  list())
            cmd   = '%s(%s)' % (exe, ','.join(args))

            self._prof.prof('task_get', comp=wid, uid=tid)

            def breakdown(cpt):
                
                import re
                import hostlist
                self._log.debug('Entering breakdown function: ') 
                node_list = os.environ['SLURM_NODELIST']
                slurm_nodes = hostlist.expand_hostlist(node_list)
                host_list = []
                cores_per_task = cpt
                cores_per_node = os.environ['SLURM_CPUS_ON_NODE']
                nodes_per_task = int(cores_per_task/int(cores_per_node))
                func_id = os.environ['RP_FUNCS_ID'] 
                exec_id = int(re.sub("^0+(?!$)", "", func_id.split('.')[1]))

                dual_nodes = [*zip( slurm_nodes[::2],  slurm_nodes[1::2])]
                task_matrix = [list(elem) for elem in dual_nodes]
                for i in range(nodes_per_task):
                    for j in range(int(cores_per_node)):
                        host_list.append(task_matrix[exec_id][i])
                hosts = ", ".join(repr(e) for e in host_list)
                
                return hosts


            def check_obj(obj):
                '''
                Check the object type if pickled or not
                to distinguish between exec and eval
                '''
                try:
                    obj = pickle.loads(codecs.decode(exe.encode(),"base64"))
                    if isinstance(obj, dict) is True:
                        return True

                except:
                    return False

            try:
                if check_obj(exe) is True:
                    state      = None
                    result     = None
                    hosts      = None

                    # We check if the Pilot is running on a Local or remote resource
                    if os.environ['PILOT_SCHEMA'] == 'LOCAL':
                        pass
                    else:
                        cores_per_task = descr['cpu_processes']
                        hosts =  breakdown(cores_per_task)
                    mpi_kwargs = descr

                    try:
                        self._prof.prof('worker_start', comp=wid, uid=tid)
                        state, result = self._worker.launch_mpirun_func(exe, hosts, **mpi_kwargs)
                        self._prof.prof('worker_stop', comp=wid, uid=tid)
   
                    except Exception as e:
                        task['stderr'] = str(e)
                        task['state']  = 'FAILED'
                        raise

                    if state == 'DONE':
                        task['stdout'] = result
                        task['stderr'] = None
                        task['state']  = state
                    else:
                        task['stdout'] = result
                        task['stderr'] = None
                        task['state']  = state


            except Exception as e:

                    task['stdout'] = None
                    task['stderr'] = str(e)
                    task['state']  = 'FAILED'

            self._prof.prof('task_put', comp=wid, uid=tid)

            task['wid'] = wid
            self._mpq_result.put(task)


# ------------------------------------------------------------------------------
#
if __name__ == '__main__':

    tasker = Tasker()
    

# ------------------------------------------------------------------------------

