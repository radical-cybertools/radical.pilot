#!/usr/bin/env python3

import os
import sys
import dill
import time
import codecs
import pickle
from mpi4py import MPI

import radical.utils as ru

IDLE = False
BUSY = True

MPI.pickle.__init__(dill.dumps, dill.loads)


# ------------------------------------------------------------------------------
#
class MPIExecutor(object):
    '''
    This class executes MPI functions in a new, private sub-communicator.

    The class should be used as follows:

        executor = MPIExecutor()
        executor.work()

    The code above should run in *all* ranks of `MPI_COMM_WORLD`, i.e., the code
    above should run under (for example):

        mpirun -n 16 --use-hwthread-cpus executor.py

    The class will run `self.work_master()` on rank 0, and `self.work_worker()`
    on all other ranks.

    Master:
    -------

    The master then iterates over incoming tasks (well, over a static task list
    at the moment) and will try to find as many idle workers as the task
    requires (`task['ranks']`).  It then sends a direct MPI message (tag `0`) to
    exactly those workers.

    Once all tasks are sent, the master will collect all task results (`recv`
    with message tag `1`).  Once all results are received, the master will send
    a termination message to all workers (tag `0`, message `None`), and will
    terminate -- as will the workers when receiving that message.


    Worker:
    -------

    The worker will wait for an incoming message with tag `0` (task to run).
    From the received task it will determine with what other workers it should
    create a  sub-communicator.  The respective process group is created as is
    the new communicator, and the workload (the methods `self.test_1` or
    `self.test_2`) are called, passing the sub-communicator as first argument.
    The methods can then use the communicator to communicate, and will
    eventually return.  Whatever value rank `0` of that sub-communicator will
    return is stored as result of the task, and the worker of that rank `0` will
    send the task back to the master (tag `1`).

    If the worker receives a `None` message, it will terminate.
    '''

    # --------------------------------------------------------------------------
    #
    def __init__(self):

        # ensure we have a communicator
        self._world      = MPI.COMM_WORLD
        self._group      = self._world.Get_group()
        self._pwd        = os.getcwd()
        self._uid        = os.environ.get('RP_FUNCS_ID')
        self._log        = ru.Logger(self._uid,   ns='radical.pilot', path=self._pwd)
        self._prof       = ru.Profiler(self._uid, ns='radical.pilot', path=self._pwd)
        self._host      = None
        self._port      = None
        try:
            self._host      = os.environ['REDIS_HOST']
            self._port      = int(os.environ['REDIS_PORT'])
        except KeyError:
            pass

        if self._world.rank == 0:
            self._enable_redis = False
            if self._host and self._port:
                from colmena.redis.queue import RedisQueue
                self.redis  = RedisQueue(self._host, self._port, 
                                        topics = ['rp task queue', 'rp result queue'])
                self._enable_redis = True


    # --------------------------------------------------------------------------
    #
    @property
    def rank(self) -> int:
        return self._world.rank

    @property
    def size(self) -> int:
        return self._world.size

    @property
    def master(self) -> bool:
        return bool(self.rank == 0)

    @property
    def worker(self) -> bool:
        return bool(self.rank != 0)

    # --------------------------------------------------------------------------
    #
    def _out(self, msg):
        '''
        small helper for debug output
        '''

        if self.master: self._log.debug('=== %3d: %s' % (self.rank, msg))
        else          : self._log.debug('  - %3d: %s' % (self.rank, msg))


    # --------------------------------------------------------------------------
    #
    def work(self):
        '''
        master (rank == 0) does master stuff
        worker (rank != 0) does worker stuff
        '''

        if self.master:
            self._prof.prof('work_start', uid = self._uid)
            self.single_work_master()
            self._prof.prof('work_stop', uid = self._uid)
        else:
            self.work_worker()


    # --------------------------------------------------------------------------
    #
    def single_work_master(self):
        # keep track of busy and idle workers
        resources = [IDLE] * (self.size - 1)

        MAX_WAIT_TIME = 100

        # define a static list of tasks and send single one per time to the workers
        # We assume in this mode that the task is large enough where:
        #    task[ranks] == self.resources
        #    uid  : recieved task id
        #    ranks: how many workers to use in the task's communicator
        #    work : what method to call
        #    args : what arguments to pass to the method

        # AM: the master is *always* rank 0, so why the if?
    
        _cfg      = ru.read_json('%s/%s.cfg' % (self._pwd, self._uid))

        addr_req  = _cfg.get('req_get')
        addr_res  = _cfg.get('res_put')
        addr_ctrl = _cfg.get('ctrl')

        assert(addr_req)
        assert(addr_res)
        assert(addr_ctrl)


        _zmq_req  = ru.zmq.Getter(channel='funcs_req_queue', url=addr_req)
        _zmq_res  = ru.zmq.Putter(channel='funcs_res_queue', url=addr_res)
        _zmq_ctrl = ru.zmq.Getter(channel='control_pubsub',  url=addr_ctrl)

        # make sure we are in redis mode
        if self._enable_redis:
            # connect to redis server
            self.redis.connect()

        while True:
            self._log.debug(MAX_WAIT_TIME)
            tasks = _zmq_req.get_nowait(1000)
            if tasks:
                MAX_WAIT_TIME = 100
                for task in tasks:
                    self._log.debug('got %s', task['uid'])

                    task_descr    = task['description']
                    task_exe      = task_descr['executable']
                    task['ranks'] = task_descr['cpu_processes']

                    if task['name'] == 'colmena' and self._enable_redis:
                        # make sure we are conneced
                        assert(self.redis.is_connected)
                        # pull from redis queue if we have a colmena task
                        redis_task = self.redis.get(topic = 'rp task queue')[1]
                        if redis_task:
                            self._log.debug('got a colmena tasks')
                            task_exe = redis_task
                        else:
                            continue
                    try:
                        
                        self._prof.prof('func_prep_start', uid= task['uid'])
                        if task['name'] == 'colmena':
                            task['work'] = eval(task_exe)
                        else:
                            task_info = pickle.loads(codecs.decode(task_exe.encode(),"base64"))

                            task['work']   = task_info["_cud_code"]
                            task['args']   = task_info["_cud_args"]
                            task['kwargs'] = task_info["_cud_kwargs"]
                        self._prof.prof('func_prep_stop', uid= task['uid'])

                    except Exception as e:
                        self._log.debug('ERROR %s', str(e))
                    
                    self._prof.prof('func_aloc_start', uid= task['uid'])
                    # find workers and mark as busy
                    workers = list()
                    for idx, worker in enumerate(resources):
                        if worker == IDLE:
                            workers.append(idx + 1)
                            resources[idx] = BUSY
                            if len(workers) == task['ranks']:
                                self._log.debug('enough workers found')
                                break
                    self._prof.prof('func_aloc_stop', uid= task['uid'])

                    # make sure we did find workers!
                    assert(len(workers) == task['ranks'])

                    # workers need to know who is part of the sub-communicator
                    task['workers'] = workers

                    # send the task to each of the workers
                    self._log.debug('send task %s  to  %s' % (task['uid'], workers))
                    self._prof.prof('func_start', uid= task['uid'])
                    for worker in workers:
                        self._world.send(task, dest=worker, tag=0)

                    # once a single task is sent out, collect results
                    task = self._world.recv(tag=1)
                    self._log.debug('recv %s with state %s' % (task['uid'],
                                                                task['state']))
                    self._prof.prof('func_stop', uid= task['uid'])
                    if task['name'] == 'colmena' and self._enable_redis:
                        assert(self.redis.is_connected)
                        self.redis.put(task['stdout'], topic='rp result queue')
                        task['stdout'] = 'redirected_to_redis'
                        task['description']['executable'] = None
                        self._log.debug('task_redis_put')
                    _zmq_res.put(task)

                    # once the task executing is finished mark the workers as IDLE
                    idle_workers = []
                    self._prof.prof('func_daloc_start', uid= task['uid'])
                    for idx, worker in enumerate(resources):
                        if worker == BUSY:
                            resources[idx] = IDLE
                            self._log.debug('marked worker number %d as IDLE' % (idx))
                            idle_workers.append(idx)
                            if  len(idle_workers) == task['ranks']:
                                break
                    self._prof.prof('func_daloc_stop', uid=task['uid'])

            if not tasks and MAX_WAIT_TIME != 0:
                MAX_WAIT_TIME -= 1
                time.sleep(0.1)
                continue

            # If the countdown is 0 but some/all resource still BUSY
            # then wait for them to become IDLE
            if not tasks and MAX_WAIT_TIME == 0:
                self._log.debug("waiting for resource to be IDLE to terminate")
                if all(worker == IDLE for worker in resources):
                    break
                else:
                    time.sleep(0.1)
                    continue
        # once all results are received, send termination signal to workers
        for worker in range(self.size - 1):
            self._log.debug('send term to %s' % (worker + 1))
            self._world.send(None, dest=worker + 1, tag=0)


    # --------------------------------------------------------------------------
    #
    def work_worker(self):

        # wait for termination message
        while True:

            task = self._world.recv(source=0, tag=0)

            if not task:
                self._log.debug('terminate!')
                break

            comm  = None
            group = None
            self._log.debug('recv task %s from Master >>>' % (task['uid']))
            assert(self.rank in task['workers'])

            try:
                # create new communicator with all workers assigned to this task
                group = self._group.Incl(task['workers'])
                comm  = self._world.Create_group(group)
                assert(comm)

                # work on task
                if task['name'] == 'colmena':

                    func = MPI.pickle.loads(task['work'])
                    task['args']   = func['args']
                    task['kwargs'] = func['kwargs']
                    func           = func['func']
                else:
                    func = MPI.pickle.loads(task['work'])
                    
                result = func(*task['args'], **task['kwargs'])

                # result is only reported back by rank 0 (of sub-communicator)
                if comm.rank == 0:
                    # FIXME: we need to send the result back in the queue pickled
                    # **always! and then unpickle in the client side.
                    if not result or isinstance(result, (tuple, list, set, int, float, dict)):
                        task['stdout'] = result
                    else:
                        task['stdout'] = str(MPI.pickle.dumps(result))

                    task['state'] = "DONE"
                    task['args']  = None
                    task['work']  = None
                    self._log.debug('send %s result to master <<< ' % (task['uid']))
                    self._world.send(task, dest=0, tag=1)

            except Exception as e:
                if comm.rank == 0:
                    task['stderr'] = str(e)
                    task['stdout'] = None
                    task['state']  = "FAILED"
                    task['args']   = None
                    task['work']   = None

                self._log.error('%s %s (%s)' % (task['uid'], task['state'],
                                                task['stderr']))
                self._world.send(task, dest=0, tag=1)
                raise

            finally:
                # sub-communicator must always be destroyed
                if group: group.Free()
                if comm : comm.Free()


# ------------------------------------------------------------------------------
#
if __name__ == '__main__':

    executor = MPIExecutor()
    executor.work()


# ------------------------------------------------------------------------------
